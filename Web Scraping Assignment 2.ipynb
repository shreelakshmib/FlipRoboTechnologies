{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.common.exceptions import NoSuchElementException,ElementNotInteractableException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------\n",
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name,\n",
    "experience_required. You have to scrape first 10 jobs data from  https://www.naukri.com/\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function definition\n",
    "def find_jobs_from(url) :\n",
    "    driver = webdriver.Edge(\"msedgedriver.exe\")\n",
    "    driver.get(url)\n",
    "\n",
    "    skill_field = driver.find_element_by_name('keyword')\n",
    "    skill_field.send_keys(\"Data Analyst\")\n",
    "\n",
    "    location_field = driver.find_element_by_name('location')\n",
    "    location_field.send_keys(\"Bangalore\")\n",
    "\n",
    "    search_button = driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "    try:\n",
    "        search_button.click()\n",
    "    except ElementNotInteractableException:\n",
    "        print(\"Search button Not responding!!\")\n",
    "\n",
    "    current_url = driver.current_url\n",
    "\n",
    "    driver.get(current_url)\n",
    "\n",
    "    job_title = []\n",
    "    job_location = []\n",
    "    company_name = []\n",
    "    experience_required = []\n",
    "\n",
    "    jobs = driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "    for i in range(0,len(jobs)):\n",
    "        if i < 10:\n",
    "            try:\n",
    "                job_title.append(jobs[i].text)\n",
    "            except NoSuchElementException:\n",
    "                job_title.append(\" -- \")\n",
    "\n",
    "    locations = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span\")\n",
    "    for j in range(0,len(locations)):\n",
    "        if j < 10:\n",
    "            try:\n",
    "                job_location.append(locations[j].get_attribute('innerHTML'))\n",
    "            except NoSuchElementException:\n",
    "                job_location.append(\" -- \")\n",
    "\n",
    "    experience = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span\")\n",
    "    for k in range(0,len(experience)):\n",
    "        if k < 10 :\n",
    "            try:\n",
    "                experience_required.append(experience[k].get_attribute('innerHTML'))\n",
    "            except NoSuchElementException:\n",
    "                experience_required.append(\" -- \")\n",
    "\n",
    "    company = driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "    for l in range(0,len(company)):\n",
    "        if l < 10 :\n",
    "            try:\n",
    "                company_name.append(company[l].get_attribute('innerHTML'))\n",
    "            except NoSuchElementException:\n",
    "                company_name.append(\" -- \")\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "\n",
    "    data_analyst_jobs = pd.DataFrame({\"Job Title\" : job_title,\n",
    "                                      'Job Location' : job_location,\n",
    "                                      'Experience Required' : experience_required,\n",
    "                                      'Company' : company_name\n",
    "                                     })\n",
    "    return data_analyst_jobs\n",
    "\n",
    "#Call Function\n",
    "df = find_jobs_from(\"https://www.naukri.com/\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "Q2: Write a python program to scrape data for “Data Scientist” Job position in\n",
    "“Bangalore” location. You have to scrape the job-title, job-location,\n",
    "company_name, full job-description. You have to scrape first 10 jobs data from https://www.naukri.com/.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function definition\n",
    "def find_jobs_from(url) :\n",
    "    driver = webdriver.Edge(\"msedgedriver.exe\")\n",
    "    driver.get(url)\n",
    "\n",
    "    skill_field = driver.find_element_by_name('keyword')\n",
    "    skill_field.send_keys(\"Data Scientist\")\n",
    "\n",
    "    location_field = driver.find_element_by_name('location')\n",
    "    location_field.send_keys(\"Bangalore\")\n",
    "\n",
    "    search_button = driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "    try:\n",
    "        search_button.click()\n",
    "    except ElementNotInteractableException:\n",
    "        print(\"Search button Not responding!!\")\n",
    "\n",
    "    current_url = driver.current_url\n",
    "\n",
    "    driver.get(current_url)\n",
    "\n",
    "    job_title = []\n",
    "    job_location = []\n",
    "    company_name = []\n",
    "    description = []\n",
    "\n",
    "    jobs = driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "    for i in range(0,len(jobs)):\n",
    "        if i < 10:\n",
    "            try:\n",
    "                job_title.append(jobs[i].text)\n",
    "            except NoSuchElementException:\n",
    "                job_title.append(\" -- \")\n",
    "\n",
    "    locations = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span\")\n",
    "    for j in range(0,len(locations)):\n",
    "        if j < 10:\n",
    "            try:\n",
    "                job_location.append(locations[j].get_attribute('innerHTML'))\n",
    "            except NoSuchElementException:\n",
    "                job_location.append(\" -- \")\n",
    "\n",
    "    company = driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "    for l in range(0,len(company)):\n",
    "        if l < 10 :\n",
    "            try:\n",
    "                company_name.append(company[l].get_attribute('innerHTML'))\n",
    "            except NoSuchElementException:\n",
    "                company_name.append(\" -- \")\n",
    "                \n",
    "    desc_url = []\n",
    "    description_url = driver.find_elements_by_xpath(\"//div[@class='info fleft']/a\")\n",
    "    for k in range(0,len(description_url)):\n",
    "        if k < 10 :\n",
    "            desc_url.append(description_url[k].get_attribute('href'))\n",
    "        \n",
    "    for each in desc_url:\n",
    "        try:\n",
    "            driver.get(each)\n",
    "            desc = driver.find_element_by_xpath(\"//section[@class='job-desc']\")\n",
    "            description.append(desc.text.replace(\"Job description\",\"\").replace(\"\\n\",\" \"))\n",
    "        except NoSuchElementException:\n",
    "            description.append(\"--\")\n",
    "\n",
    " \n",
    "    driver.close()\n",
    "\n",
    "\n",
    "    data_analyst_jobs = pd.DataFrame({\"Job Title\" : job_title,\n",
    "                                      'Job Location' : job_location,\n",
    "                                      'Company' : company_name,\n",
    "                                      'Job Description' : description\n",
    "                                     })\n",
    "    return data_analyst_jobs\n",
    "\n",
    "#Call Function\n",
    "df = pd.DataFrame()\n",
    "df = find_jobs_from(\"https://www.naukri.com/\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "Q3. Find job-title, job-location, company_name, and experience_required. You have to scrape first 10 jobs data from \n",
    "https://www.naukri.com/ .The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_jobs_from(url):\n",
    "    driver = webdriver.Edge(\"msedgedriver.exe\")\n",
    "    driver.get(url)\n",
    "\n",
    "    skill_field = driver.find_element_by_name('keyword')\n",
    "    skill_field.send_keys(\"Data Scientist\")\n",
    "\n",
    "    search_button = driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "    try:\n",
    "        search_button.click()\n",
    "    except ElementNotInteractableException:\n",
    "        print(\"Search button Not responding!!\")\n",
    "\n",
    "    current_url = driver.current_url\n",
    "    driver.get(current_url)\n",
    "    salary = ''\n",
    "    title = '' \n",
    "    time.sleep(3)\n",
    "    try:\n",
    "        driver.find_element_by_xpath(\"//span[@title='Delhi/NCR']\").click()\n",
    "        time.sleep(2)\n",
    "        driver.find_element_by_xpath(\"//span[@title='3-6 Lakhs']\").click()\n",
    "        time.sleep(2)\n",
    "\n",
    "    except ElementNotInteractableException as err:\n",
    "        print(err)\n",
    "\n",
    "    job_titles = []\n",
    "    job_locations = []\n",
    "    job_experiences = []\n",
    "    company_names = []\n",
    "\n",
    "    jobs = driver.find_elements_by_xpath(\"//div[@class='info fleft']/a\")\n",
    "    for i in range(0,len(jobs)):\n",
    "        if i < 10:\n",
    "            try:\n",
    "                job_titles.append(jobs[i].get_attribute('innerHTML'))\n",
    "            except NoSuchElementException:\n",
    "                job_titles.append(\" -- \")\n",
    "\n",
    "\n",
    "    locations = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span\")\n",
    "    for j in range(0,len(locations)):\n",
    "        if j < 10:\n",
    "            try:\n",
    "                job_locations.append(locations[j].get_attribute('innerHTML'))\n",
    "            except NoSuchElementException:\n",
    "                job_locations.append(\" -- \")\n",
    "\n",
    "    experience = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span\")\n",
    "    for k in range(0,len(experience)):\n",
    "        if k < 10 :\n",
    "            try:\n",
    "                job_experiences.append(experience[k].get_attribute('innerHTML'))\n",
    "            except NoSuchElementException:\n",
    "                job_experiences.append(\" -- \")\n",
    "\n",
    "    company = driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "    for l in range(0,len(company)):\n",
    "        if l < 10 :\n",
    "            try:\n",
    "                company_names.append(company[l].get_attribute('innerHTML'))\n",
    "            except NoSuchElementException:\n",
    "                company_names.append(\" -- \")\n",
    "\n",
    "    data_scientist_jobs = pd.DataFrame({\"Job Title\" : job_titles,\n",
    "                                      'Job Location' : job_locations,\n",
    "                                      'Company' : company_names,\n",
    "                                      'Experience Required': job_experiences\n",
    "                                     })\n",
    "    return data_scientist_jobs\n",
    "\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "df = find_jobs_from(\"https://www.naukri.com/\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------\n",
    "Q5: Write a python program to scrape data for first 10 job results for Data scientist\n",
    "Designation in Noida location from https://www.glassdoor.co.in/Salaries/index.htm. You have to scrape company_name, No. of days\n",
    "ago when job was posted, Rating of the company.\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_jobs_from(url):\n",
    "    driver = webdriver.Edge(\"msedgedriver.exe\")\n",
    "    driver.get(url)\n",
    "\n",
    "    time.sleep(2)\n",
    "    skill_field = driver.find_element_by_class_name('keyword')\n",
    "    skill_field.send_keys(\"Data Scientist\")\n",
    "\n",
    "    location_field = driver.find_element_by_class_name('loc')\n",
    "    location_field.send_keys(Keys.CONTROL + \"a\")\n",
    "    location_field.send_keys(Keys.DELETE)\n",
    "\n",
    "    location_field.send_keys(\"Noida\")\n",
    "\n",
    "    time.sleep(0.5)\n",
    "    search_button = driver.find_element_by_xpath(\"//button[@class='gd-btn-mkt']\")\n",
    "    try:\n",
    "        search_button.click()\n",
    "    except ElementNotInteractableException:\n",
    "        print(\"Search button Not responding!!\")\n",
    "\n",
    "\n",
    "    current_url = driver.current_url\n",
    "    driver.get(current_url)\n",
    "\n",
    "    min_salary = []\n",
    "    max_salary = []\n",
    "    avg_salary = []\n",
    "    company_name = []\n",
    "    number_of_salary = []\n",
    "\n",
    "    time.sleep(2)\n",
    "    min_sal  = driver.find_elements_by_xpath(\"//div[@class='common__RangeBarStyle__values common__flex__justifySpaceBetween common__flex__container ']/span[1]\")\n",
    "    for i in range(0,len(min_sal)):\n",
    "        if i < 10:\n",
    "            try:\n",
    "                min_salary.append(min_sal[i].get_attribute('innerHTML'))\n",
    "            except NoSuchElementException as Err:\n",
    "                min_salary.append(\" -- \")\n",
    "\n",
    "    max_sal  = driver.find_elements_by_xpath(\"//div[@class='common__RangeBarStyle__values common__flex__justifySpaceBetween common__flex__container ']/span[2]\")\n",
    "    for j in range(0,len(max_sal)):\n",
    "        if j < 10:\n",
    "            try:\n",
    "                max_salary.append(max_sal[j].get_attribute('innerHTML'))\n",
    "            except NoSuchElementException as Err:\n",
    "                max_salary.append(\" -- \")\n",
    "\n",
    "    avg_sal  = driver.find_elements_by_xpath(\"//div[@class='col-2 d-none d-md-flex flex-row justify-content-end']/strong\")\n",
    "    for k in range(0,len(avg_sal)):\n",
    "        if k < 10:\n",
    "            try:\n",
    "                avg_salary.append(avg_sal[k].get_attribute('innerHTML'))\n",
    "            except NoSuchElementException as Err:\n",
    "                avg_salary.append(\" -- \")\n",
    "\n",
    "    number  = driver.find_elements_by_xpath(\"//p[@class='css-1uyte9r css-1kuy7z7 m-0 ']\")\n",
    "    for l in range(0,len(number)):\n",
    "        if l < 10:\n",
    "            try:\n",
    "                num = number[l].text.strip().replace(\"salaries\",\"\")\n",
    "                number_of_salary.append(num)\n",
    "            except NoSuchElementException as Err:\n",
    "                number_of_salary.append(\" -- \")\n",
    "\n",
    "    company = driver.find_elements_by_xpath(\"//p[@class='m-0 ']\")\n",
    "    for m in range(0,len(company)):\n",
    "        if m < 10 :\n",
    "            try:\n",
    "                company_name.append(company[m].get_attribute('innerHTML'))\n",
    "            except NoSuchElementException as err:\n",
    "                company_name.append(\" -- \")\n",
    "    \n",
    "    data_scientist_jobs = pd.DataFrame({\"Company\" : company_name,\n",
    "                                      'Number of Salary' : number_of_salary,\n",
    "                                      'Minimum Salary' : min_salary,\n",
    "                                      'Maximum Salary' : max_salary,\n",
    "                                      'Average Salary': avg_salary\n",
    "                                     })\n",
    "    time.sleep(0.5)\n",
    "    driver.close()\n",
    "    return data_scientist_jobs\n",
    "    \n",
    "    \n",
    "\n",
    "df = find_jobs_from(\"https://www.glassdoor.co.in/Salaries/index.htm\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------\n",
    "Q6. Scrape following data for first 100 Sunglasses : Brand,Product,price and discount%.\n",
    "\n",
    "----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sunglass(url):\n",
    "    driver = webdriver.Edge(\"msedgedriver.exe\")\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    #close pop_up\n",
    "    driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\").click()\n",
    "    search = driver.find_element_by_xpath(\"//*[@placeholder='Search for products, brands and more']\")\n",
    "    search.send_keys(\"Sunglasses\")\n",
    "\n",
    "\n",
    "    search = driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "    try:\n",
    "        search.click()\n",
    "    except ElementNotInteractableException:\n",
    "        print(\"Search button Not responding!!\")\n",
    "\n",
    "\n",
    "\n",
    "    brands = []\n",
    "    products = []\n",
    "    prices = []\n",
    "    discount_percentages = []\n",
    "    urls = []\n",
    "\n",
    "    start_page=0\n",
    "    end_page=2\n",
    "\n",
    "    current_url = driver.current_url\n",
    "    driver.get(current_url)\n",
    "    \n",
    "    #Find all urls for 3 pages\n",
    "    soup = BS(driver.page_source,'html.parser')\n",
    "    url = soup.find_all('a',class_=\"ge-49M\")\n",
    "    for each in url[start_page:end_page+1]:\n",
    "        urls.append(\"https://www.flipkart.com\"+each.get('href'))\n",
    "\n",
    "    for i in urls:\n",
    "        driver.get(i)\n",
    "        brand = driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "        for j in range(0,len(brand)):\n",
    "            try:\n",
    "                if len(brands) != 100:\n",
    "                    brands.append(brand[j].text)\n",
    "            except NoSuchElementException:\n",
    "                brands.append(\" -- \")\n",
    "\n",
    "        product = driver.find_elements_by_xpath(\"//a[contains(@class,'IRpwTa') or contains(@class,'IRpwTa _2-ICcC')]\")\n",
    "        for k in range(0,len(product)):\n",
    "            try:\n",
    "                if len(products) != 100:\n",
    "                    products.append(product[k].text)\n",
    "            except NoSuchElementException:\n",
    "                product.append(\" -- \")\n",
    "\n",
    "        price = driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "        for l in range(0,len(price)):\n",
    "            try:\n",
    "                if len(prices) != 100:\n",
    "                    prices.append(price[l].get_attribute('innerHTML'))\n",
    "            except NoSuchElementException:\n",
    "                prices.append(\" -- \")\n",
    "\n",
    "        percentage = driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']/span\")\n",
    "        for m in range(0,len(percentage)):\n",
    "            try:\n",
    "                if len(discount_percentages) != 100:\n",
    "                    discount_percentages.append(percentage[m].get_attribute('innerHTML'))\n",
    "            except NoSuchElementException:\n",
    "                discount_percentages.append(\" -- \")\n",
    "                \n",
    "    \n",
    "    sunglasses = pd.DataFrame({\"Brand\" : brands,\n",
    "                                      'Product Description' : products,\n",
    "                                      'Price' : prices,\n",
    "                                      'Discount' : discount_percentages\n",
    "\n",
    "                                     })\n",
    "    time.sleep(0.5)\n",
    "    driver.close()\n",
    "    return sunglasses\n",
    "\n",
    "\n",
    "df = sunglass(\"https://www.flipkart.com/\")\n",
    "df   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "Q7: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: \n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb-includesearpods-poweradapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iphone(url):\n",
    "    driver = webdriver.Edge(\"msedgedriver.exe\")\n",
    "    driver.get(url)\n",
    "\n",
    "    #View all reviews :\n",
    "    reviews = driver.find_element_by_xpath(\"//div[@class='col JOpGWq']/a\").get_attribute('href')\n",
    "    time.sleep(0.5)\n",
    "    driver.get(reviews)\n",
    "\n",
    "    ratings = []\n",
    "    review_summary = []\n",
    "    full_review = []\n",
    "\n",
    "    urls=[]\n",
    "    \n",
    "    start_page = 1\n",
    "    end_page = 10\n",
    "\n",
    "    #Find all urls for 10 pages\n",
    "    soup = BS(driver.page_source,'html.parser')\n",
    "    url = soup.find_all('a',class_='ge-49M')\n",
    "    current_url = driver.current_url\n",
    "    urls.append(current_url)\n",
    "    for each in url[start_page:end_page+1]:\n",
    "        urls.append(\"https://www.flipkart.com\"+each.get('href'))\n",
    "\n",
    "    for every_url in urls:\n",
    "        driver.get(every_url)\n",
    "        \n",
    "        rating = driver.find_elements_by_xpath(\"//div[contains(@class,'_3LWZlK _1BLPMq') or contains(@class,'_3LWZlK _1rdVr6 _1BLPMq')]\")\n",
    "        for each in rating:\n",
    "            try:\n",
    "                if len(ratings) != 100:\n",
    "                    ratings.append(each.get_attribute('innerHTML').split(\"<\")[0])\n",
    "            except NoSuchElementException:\n",
    "                ratings.append(\"--\")\n",
    "\n",
    "        summary = driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "        for each in summary:\n",
    "            try:\n",
    "                if len(review_summary) != 100:\n",
    "                    review_summary.append(each.text)\n",
    "            except NoSuchElementException:\n",
    "                review_summary.append(\"--\")\n",
    "\n",
    "        full_reviews = driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']/div/div\")\n",
    "        for each in full_reviews:\n",
    "            try:\n",
    "                if len(full_review) != 100:\n",
    "                    full_review.append(each.text)\n",
    "            except NoSuchElementException:\n",
    "                full_review.append(\"--\")\n",
    "                \n",
    "    \n",
    "    \n",
    "    iphone_reviews = pd.DataFrame({'Ratings' : ratings,\n",
    "                                   'Review Summary' : review_summary,\n",
    "                                   'Full Review' : full_review\n",
    "                                  })\n",
    "    \n",
    "    driver.close()\n",
    "\n",
    "    return iphone_reviews\n",
    "\n",
    "        \n",
    "df = iphone(url = \"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------\n",
    "Q8: Scrape data for first 100 sneakers you find when you visit flipkart.com and\n",
    "search for “sneakers” in the search field.\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sneakers(url):\n",
    "    driver = webdriver.Edge(\"msedgedriver.exe\")\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    #close pop_up\n",
    "    driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\").click()\n",
    "    search = driver.find_element_by_xpath(\"//*[@placeholder='Search for products, brands and more']\")\n",
    "    search.send_keys(\"Sneakers\")\n",
    "\n",
    "\n",
    "    search = driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "    try:\n",
    "        search.click()\n",
    "    except ElementNotInteractableException:\n",
    "        print(\"Search button Not responding!!\")\n",
    "\n",
    "\n",
    "\n",
    "    brands = []\n",
    "    products = []\n",
    "    prices = []\n",
    "    discount_percentages = []\n",
    "    urls = []\n",
    "\n",
    "    start_page=0\n",
    "    end_page=2\n",
    "\n",
    "    current_url = driver.current_url\n",
    "    driver.get(current_url)\n",
    "    \n",
    "    #Find all urls for 3 pages\n",
    "    soup = BS(driver.page_source,'html.parser')\n",
    "    url = soup.find_all('a',class_=\"ge-49M\")\n",
    "    for each in url[start_page:end_page+1]:\n",
    "        urls.append(\"https://www.flipkart.com\"+each.get('href'))\n",
    "\n",
    "    for i in urls:\n",
    "        driver.get(i)\n",
    "        brand = driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "        for j in range(0,len(brand)):\n",
    "            try:\n",
    "                if len(brands) != 100:\n",
    "                    brands.append(brand[j].text)\n",
    "            except NoSuchElementException:\n",
    "                brands.append(\" -- \")\n",
    "\n",
    "        product = driver.find_elements_by_xpath(\"//a[contains(@class,'IRpwTa') or contains(@class,'IRpwTa _2-ICcC')]\")\n",
    "        for k in range(0,len(product)):\n",
    "            try:\n",
    "                if len(products) != 100:\n",
    "                    products.append(product[k].text)\n",
    "            except NoSuchElementException:\n",
    "                product.append(\" -- \")\n",
    "\n",
    "        price = driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "        for l in range(0,len(price)):\n",
    "            try:\n",
    "                if len(prices) != 100:\n",
    "                    prices.append(price[l].get_attribute('innerHTML'))\n",
    "            except NoSuchElementException:\n",
    "                prices.append(\" -- \")\n",
    "\n",
    "        percentage = driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']/span\")\n",
    "        for m in range(0,len(percentage)):\n",
    "            try:\n",
    "                if len(discount_percentages) != 100:\n",
    "                    discount_percentages.append(percentage[m].get_attribute('innerHTML'))\n",
    "            except NoSuchElementException:\n",
    "                discount_percentages.append(\" -- \")\n",
    "                \n",
    "    \n",
    "    sunglasses = pd.DataFrame({\"Brand\" : brands,\n",
    "                                      'Product Description' : products,\n",
    "                                      'Price' : prices,\n",
    "                                      'Discount' : discount_percentages\n",
    "\n",
    "                                     })\n",
    "    time.sleep(0.5)\n",
    "    driver.close()\n",
    "    return sunglasses\n",
    "\n",
    "\n",
    "df = find_sneakers(\"https://www.flipkart.com/\")\n",
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------\n",
    "Q9. Go to the link - https://www.myntra.com/shoes. Set Price filter to “Rs. 6649 to Rs. 13099” , Color filter to “Black”. \n",
    "And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe description and price of the shoe\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_shoes(url):\n",
    "    driver = webdriver.Edge(\"msedgedriver.exe\")\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "\n",
    "    #Filter : Price Range : Rs 5899 - Rs 11599\n",
    "    driver.find_element_by_xpath(\"//*[@id='mountRoot']/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label/div\").click()\n",
    "    time.sleep(1)\n",
    "    current_url = driver.current_url\n",
    "    driver.get(current_url)\n",
    "    #Filter : Color : Black\n",
    "    driver.find_element_by_xpath(\"//*[@id='mountRoot']/div/div[1]/main/div[3]/div[1]/section/div/div[6]/ul/li[1]/label/div\").click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    start_page = 0\n",
    "    end_page = 1\n",
    "\n",
    "    current_url = driver.current_url\n",
    "    driver.get(current_url)\n",
    "\n",
    "    brands = []\n",
    "    descriptions = []\n",
    "    prices = []\n",
    "\n",
    "    for p in range(start_page,end_page+1):\n",
    "        brand = driver.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "        for i in brand:\n",
    "            try:\n",
    "                if len(brands) != 100:\n",
    "                    brands.append(i.text)\n",
    "            except NoSuchElementException:\n",
    "                brands.append(\" -- \")\n",
    "\n",
    "        desc =  driver.find_elements_by_xpath(\"//h4[@class='product-product']\")\n",
    "        for m in desc:\n",
    "            try:\n",
    "                if len(descriptions) != 100:\n",
    "                    descriptions.append(m.text)\n",
    "            except NoSuchElementException:\n",
    "                descriptions.append(\" -- \")\n",
    "\n",
    "        price = driver.find_elements_by_xpath(\"//div[@class='product-price']\")\n",
    "        for l in price:\n",
    "            try:\n",
    "                if len(prices) != 100:\n",
    "                    prices.append(l.text.split(\"Rs.\")[1])\n",
    "            except NoSuchElementException:\n",
    "                prices.append(\" -- \")     \n",
    "\n",
    "\n",
    "        next_url = driver.find_element_by_xpath(\"//*[@id='desktopSearchResults']/div[2]/section/div[2]/ul/li[12]/a\")\n",
    "        driver.get(next_url.get_attribute('href'))\n",
    "\n",
    "    sneakers = pd.DataFrame({'Brand' : brands,\n",
    "                                'Descriptions': descriptions,\n",
    "                                'Price (in Rs)': prices})\n",
    "    \n",
    "    driver.close()\n",
    "    \n",
    "    return sneakers\n",
    "\n",
    "df = find_shoes(\"https://www.myntra.com/shoes\")\n",
    "df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "Q10: Go to webpage https://www.amazon.in/ . Enter “Laptop” in the search field and then click the search icon.\n",
    " Then set CPU Type filter to “Intel Core i7” and “Intel Core i9”. After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes\n",
    "for each laptop: title, Ratings and Price.\n",
    "    \n",
    "----------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_laptops(url):\n",
    "    driver = webdriver.Edge(\"msedgedriver.exe\")\n",
    "    driver.get(url)\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    search = driver.find_element_by_xpath(\"//input[@id='twotabsearchtextbox']\")\n",
    "    search.send_keys(\"Laptop\")\n",
    "\n",
    "\n",
    "    search = driver.find_element_by_xpath(\"//input[@id='nav-search-submit-button']\")\n",
    "    try:\n",
    "        search.click()\n",
    "        time.sleep(1)\n",
    "        \n",
    "        #Filter : CPU - Intel Core i5\n",
    "        driver.find_element_by_xpath(\"//*[@id='p_n_feature_thirteen_browse-bin/12598162031']/span/a/span\").click()\n",
    "        time.sleep(1)\n",
    "        \n",
    "        current_url = driver.current_url\n",
    "        driver.get(current_url)\n",
    "        \n",
    "        #Filter : CPU - Intel Core i7\n",
    "        driver.find_element_by_xpath(\"//*[@id='p_n_feature_thirteen_browse-bin/12598163031']/span/a/span\").click()\n",
    "        time.sleep(1)\n",
    "\n",
    "    except ElementNotInteractableException:\n",
    "        print(\"Search button Not responding!!\")\n",
    "\n",
    "\n",
    "\n",
    "    titles = []\n",
    "    ratings = []\n",
    "    prices = []\n",
    "   \n",
    "\n",
    "    title = driver.find_elements_by_xpath(\"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "    for j in range(0,len(title)):\n",
    "        try:\n",
    "            if len(titles) != 10:\n",
    "                titles.append(title[j].text)\n",
    "        except NoSuchElementException:\n",
    "            titles.append(\" -- \")\n",
    "\n",
    "    rating = driver.find_elements_by_xpath(\"//span[@class='a-icon-alt']\")\n",
    "    for k in range(0,len(rating)):\n",
    "        try:\n",
    "            if len(ratings) != 10:\n",
    "                ratings.append(rating[k].get_attribute('innerHTML').split(\" \")[0])\n",
    "        except NoSuchElementException:\n",
    "            ratings.append(\" -- \")\n",
    "\n",
    "    price = driver.find_elements_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "    for l in range(0,len(price)):\n",
    "        try:\n",
    "            if len(prices) != 10:\n",
    "                prices.append(price[l].get_attribute('innerHTML'))\n",
    "        except NoSuchElementException:\n",
    "            prices.append(\" -- \")\n",
    "\n",
    "    \n",
    "    \n",
    "    laptops = pd.DataFrame({\"Title\" : titles,\n",
    "                            'Rating ' : ratings,\n",
    "                            'Price' : prices                                      \n",
    "                            })\n",
    "    time.sleep(0.5)\n",
    "    driver.close()\n",
    "    return laptops\n",
    "\n",
    "\n",
    "df = find_laptops(\"https://www.amazon.in/\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------\n",
    "Q4: Write a python program to scrape data for first 10 job results for Data scientist\n",
    "Designation in Noida location from https://www.glassdoor.co.in/index.htm .You have to scrape company_name, No. of days\n",
    "ago when job was posted, Rating of the company.\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_jobs_from(url):\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "    \n",
    "    #sign-in\n",
    "    driver.find_element_by_xpath(\"//div[@class='locked-home-sign-in']\").click()\n",
    "    \n",
    "    print(\"To login to glassdoor website, you need to login!\")\n",
    "    email_id = input(\"Enter your Email ID :\")\n",
    "    email = driver.find_element_by_xpath(\"//input[@id='userEmail']\")\n",
    "    email.send_keys(email_id)\n",
    "    \n",
    "    password = input(\"Enter your password: \")\n",
    "    pwd = driver.find_element_by_xpath(\"//input[@id='userPassword']\")\n",
    "    pwd.send_keys(password)\n",
    "\n",
    "    driver.find_element_by_xpath(\"//button[@class='gd-ui-button minWidthBtn css-8i7bc2']\").click()\n",
    "    \n",
    "    time.sleep(2)\n",
    "    current_url = driver.current_url\n",
    "    driver.get(current_url)\n",
    "    \n",
    "    #search page\n",
    "    skill_field = driver.find_element_by_xpath(\"//input[@id='sc.keyword']\")\n",
    "    skill_field.send_keys(\"Data Scientist\")\n",
    "\n",
    "    location_field = driver.find_element_by_xpath(\"//input[@id='sc.location']\")\n",
    "    location_field.send_keys(Keys.CONTROL + \"a\")\n",
    "    location_field.send_keys(Keys.DELETE)\n",
    "    location_field.send_keys(\"Noida\")\n",
    "\n",
    "    time.sleep(0.5)\n",
    "    search_button = driver.find_element_by_xpath(\"//button[@class='gd-ui-button ml-std col-auto SearchStyles__newSearchButton css-iixdfr']\")\n",
    "    try:\n",
    "        search_button.click()\n",
    "    except ElementNotInteractableException:\n",
    "        print(\"Search button Not responding!!\")\n",
    "        \n",
    "    time.sleep(0.5)\n",
    "\n",
    "\n",
    "\n",
    "    current_url = driver.current_url\n",
    "    driver.get(current_url)\n",
    "\n",
    "    companies = []\n",
    "    ratings = []\n",
    "    days = []\n",
    "    \n",
    "    time.sleep(1)\n",
    "    company  = driver.find_elements_by_xpath(\"//a[@class=' css-10l5u4p e1n63ojh0 jobLink']/span\")\n",
    "    for i in range(0,len(company)):\n",
    "        if i < 10:\n",
    "            try:\n",
    "                companies.append(company[i].get_attribute('innerHTML'))\n",
    "            except NoSuchElementException as Err:\n",
    "                companies.append(\" -- \")\n",
    "\n",
    "    rating  = driver.find_elements_by_xpath(\"//span[@class='compactStars ']\")\n",
    "    for j in range(0,len(rating)):\n",
    "        if j < 10:\n",
    "            try:\n",
    "                ratings.append(rating[j].get_attribute('innerHTML').split(\"<\")[0])\n",
    "            except NoSuchElementException as Err:\n",
    "                ratings.append(\" -- \")\n",
    "\n",
    "    day  = driver.find_elements_by_xpath(\"//div[@class='d-flex align-items-end pl-std css-mi55ob']\")\n",
    "    for k in range(0,len(day)):\n",
    "        if k < 10:\n",
    "            try:\n",
    "                days.append(day[k].get_attribute('innerHTML')+\" ago\")\n",
    "            except NoSuchElementException as Err:\n",
    "                days.append(\" -- \")\n",
    "\n",
    "    data_scientist_jobs = pd.DataFrame({'Company' : companies,\n",
    "                                      'Rating' : ratings,\n",
    "                                      'Job Posted' : days\n",
    "                                      \n",
    "                                 })\n",
    "    print(len(companies),len(ratings),len(days)) \n",
    "    time.sleep(0.5)\n",
    "    driver.close()\n",
    "    return data_scientist_jobs\n",
    "    \n",
    "    \n",
    "\n",
    "df = find_jobs_from(\"https://www.glassdoor.co.in/index.htm\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
